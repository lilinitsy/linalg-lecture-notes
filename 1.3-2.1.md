# 1.3: Matrices, and their relation to vectors
I don't like how Strang covers this section, so we're going to replace some of the content here, as well as push off some of the content til later (where it gets reexplained anyway.)

Let's say we have the 3x2 matrix, $A = \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix}$, and the 2x1 vector $x = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$, and we want to examine $Ax$, let's write out what this would look like as a set of equations, without matrices.

$$1x_1 + 4x_2$$
$$2x_1 + 5x_2$$
$$3x_1 + 6x_2$$

We should be able to see here that the first row of A, $\begin{bmatrix} 1 & 4 \end{bmatrix}$, multiplies by $x_1$ and $x_2$. Each **row** of A multiples by each **column** of x (there is only 1 column, of course). 

$$Ax = x_1 \begin{bmatrix}1 \\ 2 \\ 3 \end{bmatrix} + x_2 \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}$$

So, we can see that $Ax$ is **really a linear combination of the columns of A**.

This is important to notice, because linear combinations are very important in linear algebra. But, it's also important to see how the dot product comes into play here.

$$Ax = \begin{bmatrix} (1, 4) * (x_1, x_2) \\ (2, 5) * (x_1, x_2) \\ (3, 6) * (x_1, x_2)\end{bmatrix}$$

Let's try these: Compute them both using the dot product method, and also as a combination of the columns of A to figure out what the solution is. Each will will give different and important insights.

$A_1 = \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix}$, $x_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$,

$A_2 = \begin{bmatrix} 1 & 4 & 6 \\ 2 & 3 & 5 \end{bmatrix}$, $x_2 = \begin{bmatrix} -1 \\ -2 \\ 4 \end{bmatrix}$

If possible, compute $A_1 x_2$ and $A_2 x_1$. Why or why would this not be possible?

## Linear Equations
A lot of times, we will know the result of $Ax = b$, but we won't know $x$. We may want to figure out what combination of each column of A will give us a particular b; what would $x_1$, $x_2$, and $x_3$ be, for instance?

Book example: Say that $A = \begin{bmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ 0 & -1 & 1 \end{bmatrix}$ Then, $x = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}$, and a generalized $Ax = b$ looks like:

$$x_1 + 0x_2 + 0x_3 = b_1$$
$$-x_1 + x_2 + 0x_3 = b_2$$
$$0x_1 - x_2 + x_3 = b_3 $$

The simple way to solve this would be to solve for the inverse of A; that is, if $Ax = b$, solve for $x = A^{-1} b$

Later, we will examine exactly how we found the inverse A, but once we do, we will find that the solution for x looks like the following.

$$x_1 = b_1$$
$$x_2 = b_1 + b_2$$
$$x_3 = b_1 + b_2 + b_3$$

I would like to only touch on this for now. If you want more detail immediately, Strang covers it in the book...

## Linear Independence

In a matrix, we can examine whether each column is *independent*. For instance, if we have two vectors, $u$ and $v$, would a third vector $w$ allow us to get to any new points? Or would $w$ be just a combination of $u$ and $v$?

For example, in 3D, if we define $u = \begin{bmatrix} u_1 \\ u_2 \\ u_3 \end{bmatrix}$ and $v = \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix}$, we want to know if some $cw + dv = w$. Let's examine this with some examples on the whiteboard....

If $Ax = 0$ has one solution, we know that $A$ is invertible. $x = A^{-1} b$. If $Ax = 0$ has many solutions, then $A$ is a **singular** matrix; the columns are **dependent**.