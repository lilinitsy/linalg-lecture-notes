# 1.3 / 2.1: Matrices, and their relation to vectors
I don't like how Strang covers this section, so we're going to replace some of the content here, as well as push off some of the content til later (where it gets reexplained anyway.)

Let's say we have the 3x2 matrix, $A = \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix}$, and the 2x1 vector $x = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$, and we want to examine $Ax$, let's write out what this would look like as a set of equations, without matrices.

$$1x_1 + 4x_2$$
$$2x_1 + 5x_2$$
$$3x_1 + 6x_2$$

We should be able to see here that the first row of A, $\begin{bmatrix} 1 & 4 \end{bmatrix}$, multiplies by $x_1$ and $x_2$. Each **row** of A multiples by each **column** of x (there is only 1 column, of course). 

$$Ax = x_1 \begin{bmatrix}1 \\ 2 \\ 3 \end{bmatrix} + x_2 \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}$$

So, we can see that $Ax$ is **really a linear combination of the columns of A**. We can call this the **column picture.**

Given:
$$x - 2y = 1$$
$$3x + 2y = 11$$

visually, we can see that the solution is (3, 1)  by graphing. This point solves both equations, giving us a solution to the system. The solution could then be written, in terms of **columns of A**, as 

$$x \begin{bmatrix} 1 \\ 3 \end{bmatrix} + y \begin{bmatrix} -2 \\ 2 \end{bmatrix} = \begin{bmatrix} 1 \\ 11 \end{bmatrix}$$

When we examine the column picture, we can see that there is clearly a solution, because $b$ lies within a plane that is accessible via a linear combination of columns 1 and 2. Thus, giving us the linear combination above. 


This is important to notice, because linear combinations are very important in linear algebra. But, it's also important to see how the dot product comes into play here.

$$Ax = \begin{bmatrix} (1, 4) * (x_1, x_2) \\ (2, 5) * (x_1, x_2) \\ (3, 6) * (x_1, x_2)\end{bmatrix}$$

In other words...
$$Ax = \begin{bmatrix} (row 1) * (x_1, x_2) \\ (row 2) * (x_1, x_2) \\ (row 3) * (x_1, x_2)\end{bmatrix}$$

Let's try these: Compute them both using the dot product method, and also as a combination of the columns of A to figure out what the solution is. Each will will give different and important insights.

$A_1 = \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix}$, $x_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix}$,

$A_2 = \begin{bmatrix} 1 & 4 & 6 \\ 2 & 3 & 5 \end{bmatrix}$, $x_2 = \begin{bmatrix} -1 \\ -2 \\ 4 \end{bmatrix}$

If possible, compute $A_1 x_2$ and $A_2 x_1$. Why or why would this not be possible?

## Linear Equations
A lot of times, we will know the result of $Ax = b$, but we won't know $x$. We may want to figure out what combination of each column of A will give us a particular b; what would $x_1$, $x_2$, and $x_3$ be, for instance?

Book example: Say that $A = \begin{bmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ 0 & -1 & 1 \end{bmatrix}$ Then, $x = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}$, and a generalized $Ax = b$ looks like:

$$x_1 + 0x_2 + 0x_3 = b_1$$
$$-x_1 + x_2 + 0x_3 = b_2$$
$$0x_1 - x_2 + x_3 = b_3 $$

The simple way to solve this would be to solve for the inverse of A; that is, if $Ax = b$, solve for $x = A^{-1} b$

Later, we will examine exactly how we found the inverse A, but once we do, we will find that the solution for x looks like the following.

$$x_1 = b_1$$
$$x_2 = b_1 + b_2$$
$$x_3 = b_1 + b_2 + b_3$$

I would like to only touch on this for now. If you want more detail immediately, Strang covers it in the book...

## The overall picture:
Multiplication by rows is the dot product of each row of the coefficient matrix, $A$, by the solution, $x$.

But, we can also solve $Ax$ as a combination of column vectors. There, $Ax = x_1 * column_1 + ... x_n * column_n$

## Linear Independence

In a matrix, we can examine whether each column is *independent*. For instance, if we have two vectors, $u$ and $v$, would a third vector $w$ allow us to get to any new points? Or would $w$ be just a combination of $u$ and $v$?

For example, in 3D, if we define $u = \begin{bmatrix} u_1 \\ u_2 \\ u_3 \end{bmatrix}$ and $v = \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix}$, we want to know if some $cw + dv = w$. Let's examine this with some examples on the whiteboard....

If $Ax = 0$ has one solution, we know that $A$ is invertible. $x = A^{-1} b$. If $Ax = 0$ has many solutions, then $A$ is a **singular** matrix; the columns are **dependent**.